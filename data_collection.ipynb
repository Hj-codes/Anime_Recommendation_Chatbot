{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\amit singh\\AppData\\Local\\Temp\\ipykernel_13808\\586748550.py\", line 159, in crawl\n",
      "    result = self.scrape_url(url)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\amit singh\\AppData\\Local\\Temp\\ipykernel_13808\\586748550.py\", line 124, in scrape_url\n",
      "    if self.is_404_page():\n",
      "       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\amit singh\\AppData\\Local\\Temp\\ipykernel_13808\\586748550.py\", line 55, in is_404_page\n",
      "    return self.driver.current_url == \"https://anilist.co/404\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\amit singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 435, in current_url\n",
      "    return self.execute(Command.GET_CURRENT_URL)[\"value\"]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\amit singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 345, in execute\n",
      "    self.error_handler.check_response(response)\n",
      "  File \"c:\\Users\\amit singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\", line 229, in check_response\n",
      "    raise exception_class(message, screen, stacktrace)\n",
      "selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=119.0.6045.199)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7275C82B2+55298]\n",
      "\t(No symbol) [0x00007FF727535E02]\n",
      "\t(No symbol) [0x00007FF7273F05AB]\n",
      "\t(No symbol) [0x00007FF7273D0038]\n",
      "\t(No symbol) [0x00007FF727456BC7]\n",
      "\t(No symbol) [0x00007FF72746A15F]\n",
      "\t(No symbol) [0x00007FF727451E83]\n",
      "\t(No symbol) [0x00007FF72742670A]\n",
      "\t(No symbol) [0x00007FF727427964]\n",
      "\tGetHandleVerifier [0x00007FF727940AAB+3694587]\n",
      "\tGetHandleVerifier [0x00007FF72799728E+4048862]\n",
      "\tGetHandleVerifier [0x00007FF72798F173+4015811]\n",
      "\tGetHandleVerifier [0x00007FF7276647D6+695590]\n",
      "\t(No symbol) [0x00007FF727540CE8]\n",
      "\t(No symbol) [0x00007FF72753CF34]\n",
      "\t(No symbol) [0x00007FF72753D062]\n",
      "\t(No symbol) [0x00007FF72752D3A3]\n",
      "\tBaseThreadInitThunk [0x00007FFEA9A7257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFEAB36AA58+40]\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\amit singh\\AppData\\Local\\Temp\\ipykernel_13808\\586748550.py\", line 188, in crawl\n",
      "    raise RuntimeWarning\n",
      "RuntimeWarning\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "import json\n",
    "import traceback\n",
    "import selenium.common.exceptions\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "class AnimeScraper:\n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        # self.chrome_options = Options()\n",
    "\n",
    "    def load_sites_data(self, filename='Anime_data.json'):\n",
    "        try:\n",
    "            # Read Anime_data.json\n",
    "            with open(filename, 'r', encoding='utf-8') as anime_file:\n",
    "                anime_data = json.load(anime_file)\n",
    "            \n",
    "            visited_sites = set()\n",
    "            unvisited_sites = set()\n",
    "            for entry in anime_data:\n",
    "                \n",
    "                visited = entry.get(\"visited\")\n",
    "                unvisited = entry.get(\"unvisited\")\n",
    "                \n",
    "                if visited:\n",
    "                    visited_sites.update(visited)\n",
    "                if unvisited:\n",
    "                    unvisited_sites.update(unvisited)\n",
    "\n",
    "            return visited_sites, unvisited_sites\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            return set(), set()\n",
    "\n",
    "    def init_driver(self):\n",
    "        # self.chrome_options.add_argument(\"--headless\")\n",
    "        # self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver = webdriver.Chrome()\n",
    "\n",
    "    def close_driver(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "    def visit_url(self, url):\n",
    "        if self.driver:\n",
    "            self.driver.get(url)\n",
    "            sleep(2)\n",
    "\n",
    "    def is_404_page(self):\n",
    "        if self.driver:\n",
    "            return self.driver.current_url == \"https://anilist.co/404\"\n",
    "        return False\n",
    "\n",
    "    def find_links(self, max_tries):\n",
    "        if self.driver:\n",
    "            try:\n",
    "                for _ in range(max_tries):\n",
    "                    # Find all links inside the 'cover-link' class divs\n",
    "                    link_elements = self.driver.find_elements(By.CLASS_NAME, 'cover-link')\n",
    "                    urls = [link.get_attribute('href') for link in link_elements]\n",
    "                    valid_urls = [url for url in urls if \"/anime/\" in url]\n",
    "                    if len(valid_urls) != 0:\n",
    "                        return valid_urls\n",
    "                    else:\n",
    "                        sleep(0.1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error while finding links: {e}\")\n",
    "        return []\n",
    "\n",
    "    def isdriveralive(self):\n",
    "        try:\n",
    "            self.driver.current_url\n",
    "            # or driver.title\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def click_spoiler_button(self):\n",
    "        if self.driver:\n",
    "            try:\n",
    "                button = self.driver.find_element(By.CLASS_NAME, \"spoiler-toggle\")\n",
    "                button.click()\n",
    "                sleep(1)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def parse_popup(self):\n",
    "        try:\n",
    "            if self.driver:\n",
    "                popup_html = self.driver.page_source\n",
    "                popup_soup = BeautifulSoup(popup_html, 'html.parser')\n",
    "\n",
    "                title = popup_soup.find(\"h1\").get_text(strip=True)\n",
    "                des = popup_soup.find(\"p\", class_=\"description content-wrap\").get_text(strip=True)\n",
    "                average_score = popup_soup.find(\"div\", class_='el-tooltip data-set').find('div', class_='value').text\n",
    "                genre_tags = popup_soup.find_all('a', class_='el-tooltip name')\n",
    "                rank_tags = popup_soup.find_all('div', class_='rank')\n",
    "                status_divs = popup_soup.find_all('div', class_='status')\n",
    "                status_data = {}\n",
    "\n",
    "                # Iterate through the status divs and extract the name and amount\n",
    "                for status_div in status_divs:\n",
    "                    name = status_div.find('div', class_='name').get_text(strip=True)\n",
    "                    amount = status_div.find('div', class_='amount').get_text(strip=True).split()[0]  # Extract the number\n",
    "\n",
    "                    # Store the data in the dictionary\n",
    "                    status_data[name] = int(amount[:-5])\n",
    "\n",
    "                if title and des and genre_tags and rank_tags:\n",
    "                    genres = dict(zip([tag.get_text(strip=True) for tag in genre_tags], [rank.get_text(strip=True) for rank in rank_tags]))\n",
    "                    return title, des, genres, average_score, status_data\n",
    "                \n",
    "        except AttributeError:\n",
    "            return None\n",
    "        \n",
    "\n",
    "    def scrape_url(self, url):\n",
    "        try:\n",
    "            self.visit_url(url)\n",
    "            if self.is_404_page():\n",
    "                return \"Error404\", None, None, None, None\n",
    "            # Fetch valid links after page is fully loaded\n",
    "            valid_links = self.find_links(200)\n",
    "            self.click_spoiler_button()\n",
    "            title, des, genre, average_score, stats = self.parse_popup()\n",
    "            return title, des, genre, average_score, valid_links, stats\n",
    "        except TypeError:\n",
    "            return None        \n",
    " \n",
    "    def write_json(self, new_data, filename=\"Anime_data.json\"):\n",
    "        existing_data = []\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as file:\n",
    "                existing_data = json.load(file)\n",
    "        except:\n",
    "            existing_data = []\n",
    "        existing_data.append(new_data)\n",
    "\n",
    "        with open(filename, \"w\", encoding='utf-8') as file:\n",
    "            json.dump(existing_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def crawl(self):\n",
    "        try:\n",
    "            self.init_driver()\n",
    "            # load site data\n",
    "            self.visited_sites, self.unvisited_sites = self.load_sites_data()\n",
    "            to_visit = list(self.unvisited_sites - self.visited_sites)\n",
    "            while self.unvisited_sites and len(to_visit) != 0 and self.driver.current_url:\n",
    "                try:\n",
    "                    # Update the to_visit list after loading the saved data\n",
    "                    to_visit = list(self.unvisited_sites - self.visited_sites)\n",
    "                    if to_visit:\n",
    "                        for url in to_visit:\n",
    "                            try:\n",
    "                                result = self.scrape_url(url)\n",
    "                                if result:\n",
    "                                    title, des, genres, average_score, urls, stats = result\n",
    "                                    Anime_data = {\n",
    "                                        \"Anime\": {\n",
    "                                            \"Anime_Title\": title,\n",
    "                                            \"Description\": des,\n",
    "                                            \"Tags\": genres,\n",
    "                                            \"Average_scores\": average_score,\n",
    "                                            \"stats\": stats\n",
    "                                        },\n",
    "                                        \"unvisited\": urls,\n",
    "                                        \"visited\": url\n",
    "                                    }\n",
    "                                    self.write_json(Anime_data)\n",
    "                                    if urls:\n",
    "                                        self.unvisited_sites.update(urls)\n",
    "                                    self.visited_sites.add(url)\n",
    "                                else:\n",
    "                                    print(f\"Error scraping URL: {url}\")\n",
    "                                    self.visited_sites.add(url)  # Add the problematic URL to visited sites\n",
    "                                    if not(self.isdriveralive()):\n",
    "                                        break\n",
    "                            \n",
    "                            except selenium.common.exceptions.WebDriverException as e:\n",
    "                                if \"unknown error: net::ERR_INTERNET_DISCONNECTED\" in str(e):\n",
    "                                    raise ConnectionError\n",
    "                                \n",
    "                                elif \"Message: no such window: target window already closed\" in str(e):\n",
    "                                    raise RuntimeWarning\n",
    "                            \n",
    "                            except Exception:\n",
    "                                traceback.print_exc()\n",
    "                                self.visited_sites.add(url)  # Add the problematic URL to visited sites\n",
    "                                break\n",
    "\n",
    "                except Exception:\n",
    "                    traceback.print_exc()\n",
    "                    break\n",
    "\n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "            \n",
    "        finally:\n",
    "            self.close_driver()\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        scraper = AnimeScraper()\n",
    "        scraper.crawl()\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        scraper.close_driver()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
