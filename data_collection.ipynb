{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup(driver, call=0):\n",
    "    if call < 15:\n",
    "        try:\n",
    "            popup_html = driver.page_source\n",
    "\n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            popup_soup = BeautifulSoup(popup_html, 'html.parser')\n",
    "\n",
    "            ## Scraping relevant information from the popup using BeautifulSoup\n",
    "            # Find all <a> tags with the desired class\n",
    "            genre_tags = popup_soup.find_all('a', class_='el-tooltip name')\n",
    "\n",
    "            # Find all <div> tags with the class \"rank\"\n",
    "            rank_tags = popup_soup.find_all('div', class_='rank')\n",
    "\n",
    "            # Extract the title of the anime\n",
    "            title = popup_soup.find(\"h1\").get_text(strip=True)\n",
    "\n",
    "            # Extract the description/synopsis\n",
    "            des = popup_soup.find(\"p\", class_=\"description content-wrap\").get_text(strip=True)\n",
    "\n",
    "            average_score = popup_soup.find(\"div\", class_='el-tooltip data-set')\n",
    "\n",
    "            # Find the div element with class \"value\" and extract its text\n",
    "            value_element = average_score.find('div', class_='value')\n",
    "            value_text = value_element.text\n",
    "\n",
    "            if title != None and des != None and  genre_tags != None and  rank_tags != None:\n",
    "                # make list to save the info\n",
    "                Genre = []\n",
    "                rank = []\n",
    "\n",
    "                # Extract and print the text content of each <a> tag\n",
    "                for tag in genre_tags:\n",
    "                    genre = tag.get_text(strip=True)\n",
    "                    Genre.append(genre)\n",
    "\n",
    "                # Extract and print the text content of rank tags\n",
    "                for rank_tag in rank_tags:\n",
    "                    rank_text = rank_tag.get_text(strip=True)\n",
    "                    rank.append(rank_text)\n",
    "                genres = dict(zip(Genre,rank))\n",
    "                # print(\"try\")\n",
    "\n",
    "                return title, des, genres, value_text\n",
    "    \n",
    "        except:\n",
    "            sleep(call/5)\n",
    "            soup(driver, call=call+1)\n",
    "    else:\n",
    "        print(\"failsoup : \", driver.current_url)\n",
    "        return \"Error404\", None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(website_url, driver):\n",
    "    \n",
    "    driver.get(website_url)\n",
    "    sleep(2)\n",
    "    #print(\"scrape\")\n",
    "    \n",
    "    if driver.current_url == \"https://anilist.co/404\":\n",
    "        print(\"proc\")\n",
    "        return \"Error404\", None, None, None, None\n",
    "\n",
    "    # Extracting links from site\n",
    "    links = driver.find_elements(By.CLASS_NAME, 'cover-link')\n",
    "    if links == []:\n",
    "        x = 0\n",
    "        while len(links) == 0 and x < 4:\n",
    "            driver.get(website_url)\n",
    "            sleep(x)\n",
    "            links = driver.find_elements(By.CLASS_NAME, 'cover-link')\n",
    "            print(links)\n",
    "            x += 1\n",
    "            \n",
    "    urls = [link.get_attribute('href') for link in links]\n",
    "    urls = [link for link in urls if \"/anime/\" in link ]\n",
    "    \n",
    "    # print(\"links\")\n",
    "    try:\n",
    "        # Click the button\n",
    "        button = driver.find_element(By.CLASS_NAME, \"spoiler-toggle\")\n",
    "        button.click()\n",
    "        sleep(1)\n",
    "    except:\n",
    "        # print(\"exc\")\n",
    "        pass\n",
    "    \n",
    "    # try:\n",
    "    #     title, des, genre, average_score = soup(driver)\n",
    "    # except:\n",
    "    #     title, des, genre = \"Error404\", None, None\n",
    "    title, des, genre, average_score = soup(driver)\n",
    "    \n",
    "    return title, des, genre, average_score, urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(new_data, filename=\"Anime_data.json\"):\n",
    "    \n",
    "    # Read the existing JSON data from the file\n",
    "    existing_data = []\n",
    "    with open(filename,'r',encoding='utf-8') as file:\n",
    "        try:\n",
    "            # First we load existing data into a dict.\n",
    "            existing_data = json.load(file)\n",
    "        except:\n",
    "            existing_data = []\n",
    "    file.close()\n",
    "    # Join new_data with existing_data\n",
    "    existing_data.append(new_data)\n",
    "\n",
    "    with open(filename,\"w\",encoding='utf-8') as file:\n",
    "        \n",
    "        # convert back to json.\n",
    "        json.dump(existing_data, file,ensure_ascii=False, indent = 4)\n",
    "    \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(driver, unvisited_sites, visited_sites):\n",
    "\n",
    "    # Making a List of website to visit using BSF\n",
    "    to_visit = list(unvisited_sites - visited_sites)\n",
    "    print(to_visit)\n",
    "    # Loop to Scrape one set of to_visit links\n",
    "    for url in to_visit:\n",
    "        title, des, genres, average_score, urls = scrape(url,driver=driver)\n",
    "        if title != \"Error404\":\n",
    "            # Saving them in an list to further save it in Json\n",
    "            Anime_data = {\n",
    "                \"Anime\" : {\n",
    "                \"Anime_Title\" : title,\n",
    "                \"Description\" : des,\n",
    "                \"Tags\" : genres,\n",
    "                \"Average_scores\": average_score\n",
    "                },\n",
    "                \"unvisited\" : urls,\n",
    "                \"visited\" : url\n",
    "            }\n",
    "            # print(\"json\")\n",
    "            # Injecting scraped data into json\n",
    "            write_json(Anime_data,\"Anime_data.json\")\n",
    "\n",
    "            # Updating the unvisited and visited sets\n",
    "            if urls != None or urls != 'null' or len(urls) != 0:\n",
    "                unvisited_sites.update(urls)\n",
    "                # print(\"if\")\n",
    "            visited_sites.add(url)\n",
    "        else:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(filename='sites_data.json'):\n",
    "    # Load data from the JSON file\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        site_data = json.load(file)\n",
    "\n",
    "    # Extract the lists from the loaded data\n",
    "    try:\n",
    "        visited_sites_list = site_data.get(\"visited_sites\", [])\n",
    "    except:\n",
    "        visited_sites_list = []\n",
    "    try:\n",
    "        unvisited_sites_list = site_data.get(\"unvisited_sites\", [])\n",
    "    except:\n",
    "        visited_sites_list = []\n",
    "    \n",
    "    return visited_sites_list, unvisited_sites_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a set to keep visited & unvisited sites\n",
    "def load():\n",
    "    visited_sites, unvisited_sites = load_files()\n",
    "    visited_sites, unvisited_sites = set(visited_sites), set(unvisited_sites)\n",
    "    return visited_sites, unvisited_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper():\n",
    "\n",
    "    # Initializing Driver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Your crawling logic here\n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "    visited_sites, unvisited_sites = load()\n",
    "\n",
    "    try:\n",
    "        # visting and scraping sites\n",
    "        while True:\n",
    "            crawler(driver,unvisited_sites, visited_sites)\n",
    "    except:\n",
    "        # Create a dictionary to store both sets\n",
    "        site_data = {\n",
    "        \"visited_sites\": list(visited_sites),\n",
    "        \"unvisited_sites\": list(unvisited_sites)\n",
    "        }\n",
    "\n",
    "        # Save the dictionary to a JSON file\n",
    "        with open('sites_data.json', 'w', encoding='utf-8') as file:\n",
    "            json.dump(site_data, file)\n",
    "        \n",
    "        # Close the driver properly\n",
    "        driver.close()\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        scraper()\n",
    "    except Exception as e:\n",
    "        print(f'Exception : {e}')\n",
    "        scraper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "import json\n",
    "\n",
    "logging.basicConfig(filename='debug.log', level=logging.ERROR)\n",
    "\n",
    "def soup(driver, call=0):\n",
    "    if call < 15:\n",
    "        try:\n",
    "            popup_html = driver.page_source\n",
    "            popup_soup = BeautifulSoup(popup_html, 'html.parser')\n",
    "            genre_tags = popup_soup.find_all('a', class_='el-tooltip name')\n",
    "            rank_tags = popup_soup.find_all('div', class_='rank')\n",
    "            title = popup_soup.find(\"h1\").get_text(strip=True)\n",
    "            des = popup_soup.find(\"p\", class_=\"description content-wrap\").get_text(strip=True)\n",
    "            average_score = popup_soup.find(\"div\", class_='el-tooltip data-set')\n",
    "            value_element = average_score.find('div', class_='value')\n",
    "            value_text = value_element.text\n",
    "\n",
    "            if title != None and des != None and  genre_tags != None and  rank_tags != None:\n",
    "                Genre = []\n",
    "                rank = []\n",
    "                for tag in genre_tags:\n",
    "                    genre = tag.get_text(strip=True)\n",
    "                    Genre.append(genre)\n",
    "                for rank_tag in rank_tags:\n",
    "                    rank_text = rank_tag.get_text(strip=True)\n",
    "                    rank.append(rank_text)\n",
    "                genres = dict(zip(Genre,rank))\n",
    "                return title, des, genres, value_text\n",
    "        except Exception as e:\n",
    "            logging.error(f'Exception occurred in soup function: {e}')\n",
    "            sleep(call/5)\n",
    "            soup(driver, call=call+1)\n",
    "    else:\n",
    "        logging.info(f\"failsoup : {driver.current_url}\")\n",
    "        return \"Error404\", None, None, None\n",
    "\n",
    "def scrape(website_url, driver):\n",
    "    try:\n",
    "        driver.get(website_url)\n",
    "        sleep(2)\n",
    "        logging.info(\"scrape\")\n",
    "        if driver.current_url == \"https://anilist.co/404\":\n",
    "            logging.info(\"proc\")\n",
    "            return \"Error404\", None, None, None, None\n",
    "        links = driver.find_elements(By.CLASS_NAME, 'cover-link')\n",
    "        if links == []:\n",
    "            x = 0\n",
    "            while len(links) == 0 and x < 4:\n",
    "                driver.get(website_url)\n",
    "                sleep(x)\n",
    "                links = driver.find_elements(By.CLASS_NAME, 'cover-link')\n",
    "                logging.info(links)\n",
    "                x += 1\n",
    "        urls = [link.get_attribute('href') for link in links]\n",
    "        urls = [link for link in urls if \"/anime/\" in link ]\n",
    "        logging.info(\"links\")\n",
    "        try:\n",
    "            button = driver.find_element(By.CLASS_NAME, \"spoiler-toggle\")\n",
    "            button.click()\n",
    "            sleep(1)\n",
    "        except Exception as e:\n",
    "            logging.error(f'Exception occurred: {e}')\n",
    "            pass\n",
    "        title, des, genre, average_score = soup(driver)\n",
    "        return title, des, genre, average_score, urls\n",
    "    except Exception as e:\n",
    "        logging.error(f'Exception occurred in scrape function: {e}')\n",
    "        return \"Error404\", None, None, None, None\n",
    "\n",
    "def write_json(new_data, filename=\"Anime_data.json\"):\n",
    "    try:\n",
    "        existing_data = []\n",
    "        with open(filename,'r',encoding='utf-8') as file:\n",
    "            try:\n",
    "                existing_data = json.load(file)\n",
    "            except:\n",
    "                existing_data = []\n",
    "        file.close()\n",
    "        existing_data.append(new_data)\n",
    "        with open(filename,\"w\",encoding='utf-8') as file:\n",
    "            json.dump(existing_data, file,ensure_ascii=False, indent = 4)\n",
    "        file.close()\n",
    "    except Exception as e:\n",
    "        logging.error(f'Exception occurred in write_json function: {e}')\n",
    "\n",
    "def crawler(driver, unvisited_sites, visited_sites):\n",
    "    try:\n",
    "        to_visit = list(unvisited_sites - visited_sites)\n",
    "        logging.info(to_visit)\n",
    "        for url in to_visit:\n",
    "            title, des, genres, average_score, urls = scrape(url,driver=driver)\n",
    "            if title != \"Error404\":\n",
    "                Anime_data = {\n",
    "                    \"Anime\" : {\n",
    "                    \"Anime_Title\" : title,\n",
    "                    \"Description\" : des,\n",
    "                    \"Tags\" : genres,\n",
    "                    \"Average_scores\": average_score\n",
    "                    },\n",
    "                    \"unvisited\" : urls,\n",
    "                    \"visited\" : url\n",
    "                }\n",
    "                write_json(Anime_data,\"Anime_data.json\")\n",
    "                if urls != None or urls != 'null' or len(urls) != 0:\n",
    "                    unvisited_sites.update(urls)\n",
    "                visited_sites.add(url)\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        logging.error(f'Exception occurred in crawler function: {e}')\n",
    "\n",
    "def load_files(filename='sites_data.json'):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            site_data = json.load(file)\n",
    "        visited_sites_list = site_data.get(\"visited_sites\", [])\n",
    "        unvisited_sites_list = site_data.get(\"unvisited_sites\", [])\n",
    "        return visited_sites_list, unvisited_sites_list\n",
    "    except Exception as e:\n",
    "        logging.error(f'Exception occurred in load_files function: {e}')\n",
    "        return [], []\n",
    "\n",
    "def load():\n",
    "    try:\n",
    "        visited_sites, unvisited_sites = load_files()\n",
    "        visited_sites, unvisited_sites = set(visited_sites), set(unvisited_sites)\n",
    "        return visited_sites, unvisited_sites\n",
    "    except Exception as e:\n",
    "        logging.error(f'Exception occurred in load function: {e}')\n",
    "        return set(), set()\n",
    "\n",
    "def scraper():\n",
    "    try:\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        visited_sites, unvisited_sites = load()\n",
    "        while True:\n",
    "            crawler(driver,unvisited_sites, visited_sites)\n",
    "    except Exception as e:\n",
    "        logging.error(f'Exception occurred in scraper function: {e}')\n",
    "        site_data = {\n",
    "        \"visited_sites\": list(visited_sites),\n",
    "        \"unvisited_sites\": list(unvisited_sites)\n",
    "        }\n",
    "        with open('sites_data.json', 'w', encoding='utf-8') as file:\n",
    "            json.dump(site_data, file)\n",
    "        driver.close()\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        scraper()\n",
    "    except Exception as e:\n",
    "        logging.error(f'Exception occurred in main function: {e}')\n",
    "        scraper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "import json\n",
    "import traceback\n",
    "import selenium.common.exceptions\n",
    "\n",
    "\n",
    "class AnimeScraper:\n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.visited_sites, self.unvisited_sites = self.load_sites_data()\n",
    "\n",
    "    def load_sites_data(self, filename='Anime_data.json'):\n",
    "        try:\n",
    "            # Read Anime_data.json\n",
    "            with open(filename, 'r', encoding='utf-8') as anime_file:\n",
    "                anime_data = json.load(anime_file)\n",
    "            \n",
    "            visited_sites = {}\n",
    "            unvisited_sites = {}\n",
    "            \n",
    "            for entry in anime_data:\n",
    "                visited = entry.get(\"visited\")\n",
    "                unvisited = entry.get(\"unvisited\")\n",
    "                \n",
    "                if visited:\n",
    "                    visited_sites.update(visited)\n",
    "                if unvisited:\n",
    "                    unvisited_sites.update(unvisited)\n",
    "            print(unvisited_sites, visited_sites)\n",
    "\n",
    "            return visited_sites, unvisited_sites\n",
    "        except:\n",
    "            return set(), set()\n",
    "\n",
    "    # def save_sites_data(self, filename='sites_data.json'):\n",
    "    #     site_data = {\n",
    "    #         \"visited_sites\": list(self.visited_sites),\n",
    "    #         \"unvisited_sites\": list(self.unvisited_sites),\n",
    "    #     }\n",
    "    #     with open(filename, 'w', encoding='utf-8') as file:\n",
    "    #         json.dump(site_data, file)\n",
    "\n",
    "    def init_driver(self):\n",
    "        self.driver = webdriver.Chrome()\n",
    "\n",
    "    def close_driver(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "    def visit_url(self, url):\n",
    "        if self.driver:\n",
    "            self.driver.get(url)\n",
    "            sleep(2)\n",
    "\n",
    "    def is_404_page(self):\n",
    "        if self.driver:\n",
    "            return self.driver.current_url == \"https://anilist.co/404\"\n",
    "        return False\n",
    "\n",
    "    def find_links(self, max_tries):\n",
    "        if self.driver:\n",
    "            try:\n",
    "                for _ in range(max_tries):\n",
    "                    # Find all links inside the 'cover-link' class divs\n",
    "                    link_elements = self.driver.find_elements(By.CLASS_NAME, 'cover-link')\n",
    "                    urls = [link.get_attribute('href') for link in link_elements]\n",
    "                    valid_urls = [url for url in urls if \"/anime/\" in url]\n",
    "                    if len(valid_urls) != 0:\n",
    "                        return valid_urls\n",
    "                    else:\n",
    "                        sleep(0.1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error while finding links: {e}\")\n",
    "        return []\n",
    "\n",
    "    def isdriveralive(self):\n",
    "        try:\n",
    "            self.driver.current_url\n",
    "            # or driver.title\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def click_spoiler_button(self):\n",
    "        if self.driver:\n",
    "            try:\n",
    "                button = self.driver.find_element(By.CLASS_NAME, \"spoiler-toggle\")\n",
    "                button.click()\n",
    "                sleep(1)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def parse_popup(self):\n",
    "        try:\n",
    "            if self.driver:\n",
    "                popup_html = self.driver.page_source\n",
    "                popup_soup = BeautifulSoup(popup_html, 'html.parser')\n",
    "\n",
    "                title = popup_soup.find(\"h1\").get_text(strip=True)\n",
    "                des = popup_soup.find(\"p\", class_=\"description content-wrap\").get_text(strip=True)\n",
    "                average_score = popup_soup.find(\"div\", class_='el-tooltip data-set').find('div', class_='value').text\n",
    "                genre_tags = popup_soup.find_all('a', class_='el-tooltip name')\n",
    "                rank_tags = popup_soup.find_all('div', class_='rank')\n",
    "                status_divs = popup_soup.find_all('div', class_='status')\n",
    "                status_data = {}\n",
    "\n",
    "                # Iterate through the status divs and extract the name and amount\n",
    "                for status_div in status_divs:\n",
    "                    name = status_div.find('div', class_='name').get_text(strip=True)\n",
    "                    amount = status_div.find('div', class_='amount').get_text(strip=True).split()[0]  # Extract the number\n",
    "\n",
    "                    # Store the data in the dictionary\n",
    "                    status_data[name] = int(amount[:-5])\n",
    "\n",
    "                if title and des and genre_tags and rank_tags:\n",
    "                    genres = dict(zip([tag.get_text(strip=True) for tag in genre_tags], [rank.get_text(strip=True) for rank in rank_tags]))\n",
    "                    return title, des, genres, average_score, status_data\n",
    "                \n",
    "        except AttributeError:\n",
    "            return None\n",
    "        \n",
    "\n",
    "    def scrape_url(self, url):\n",
    "        try:\n",
    "            self.visit_url(url)\n",
    "            if self.is_404_page():\n",
    "                return \"Error404\", None, None, None, None\n",
    "            # Fetch valid links after page is fully loaded\n",
    "            valid_links = self.find_links(200)\n",
    "            self.click_spoiler_button()\n",
    "            title, des, genre, average_score, stats = self.parse_popup()\n",
    "            return title, des, genre, average_score, valid_links, stats\n",
    "        except TypeError:\n",
    "            return None        \n",
    " \n",
    "    def write_json(self, new_data, filename=\"Anime_data.json\"):\n",
    "        existing_data = []\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as file:\n",
    "                existing_data = json.load(file)\n",
    "        except:\n",
    "            existing_data = []\n",
    "        existing_data.append(new_data)\n",
    "\n",
    "        with open(filename, \"w\", encoding='utf-8') as file:\n",
    "            json.dump(existing_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def crawl(self):\n",
    "        try:\n",
    "            self.init_driver()\n",
    "            # load site data\n",
    "            self.visited_sites, self.unvisited_sites = self.load_sites_data()\n",
    "            print(self.unvisited_sites, self.visited_sites)\n",
    "            to_visit = list(self.unvisited_sites - self.visited_sites)\n",
    "            while self.unvisited_sites and len(to_visit) != 0 and self.driver.current_url:\n",
    "                try:\n",
    "                    # Update the to_visit list after loading the saved data\n",
    "                    to_visit = list(self.unvisited_sites - self.visited_sites)\n",
    "                    if to_visit:\n",
    "                        for url in to_visit:\n",
    "                            try:\n",
    "                                result = self.scrape_url(url)\n",
    "                                if result:\n",
    "                                    title, des, genres, average_score, urls, stats = result\n",
    "                                    Anime_data = {\n",
    "                                        \"Anime\": {\n",
    "                                            \"Anime_Title\": title,\n",
    "                                            \"Description\": des,\n",
    "                                            \"Tags\": genres,\n",
    "                                            \"Average_scores\": average_score,\n",
    "                                            \"stats\": stats\n",
    "                                        },\n",
    "                                        \"unvisited\": urls,\n",
    "                                        \"visited\": url\n",
    "                                    }\n",
    "                                    self.write_json(Anime_data)\n",
    "                                    if urls:\n",
    "                                        self.unvisited_sites.update(urls)\n",
    "                                    self.visited_sites.add(url)\n",
    "                                else:\n",
    "                                    print(f\"Error scraping URL: {url}\")\n",
    "                                    self.visited_sites.add(url)  # Add the problematic URL to visited sites\n",
    "                                    if not(self.isdriveralive()):\n",
    "                                        break\n",
    "                            \n",
    "                            except selenium.common.exceptions.WebDriverException as e:\n",
    "                                if \"unknown error: net::ERR_INTERNET_DISCONNECTED\" in str(e):\n",
    "                                    raise ConnectionError\n",
    "                                \n",
    "                                elif \"Message: no such window: target window already closed\" in str(e):\n",
    "                                    raise RuntimeWarning\n",
    "                            \n",
    "                            except Exception:\n",
    "                                traceback.print_exc()\n",
    "                                self.visited_sites.add(url)  # Add the problematic URL to visited sites\n",
    "                                break\n",
    "\n",
    "                except Exception:\n",
    "                    traceback.print_exc()\n",
    "                    break\n",
    "\n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "            \n",
    "        finally:\n",
    "            self.close_driver()\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        scraper = AnimeScraper()\n",
    "        scraper.crawl()\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        scraper.close_driver()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def update_sites_data():\n",
    "    # Read Anime_data.json\n",
    "    with open('Anime_data.json', 'r', encoding='utf-8') as anime_file:\n",
    "        anime_data = json.load(anime_file)\n",
    "    \n",
    "    visited_sites = []\n",
    "    unvisited_sites = []\n",
    "    \n",
    "    for entry in anime_data:\n",
    "        visited = entry.get(\"visited\")\n",
    "        unvisited = entry.get(\"unvisited\")\n",
    "        \n",
    "        if visited:\n",
    "            visited_sites.append(visited)\n",
    "        if unvisited:\n",
    "            unvisited_sites.extend(unvisited)\n",
    "\n",
    "    # Read existing sites_data.json\n",
    "    with open('sites_data.json', 'r') as sites_file:\n",
    "        sites_data = json.load(sites_file)\n",
    "\n",
    "    # Update the data in sites_data.json\n",
    "    sites_data[\"visited_sites\"] = visited_sites\n",
    "    sites_data[\"unvisited_sites\"] = unvisited_sites\n",
    "\n",
    "    # Write the updated data back to sites_data.json\n",
    "    with open('sites_data.json', 'w') as sites_file:\n",
    "        json.dump(sites_data, sites_file, indent=4)\n",
    "\n",
    "# Call the function to update sites_data.json\n",
    "update_sites_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
