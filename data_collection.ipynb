{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup(driver):\n",
    "    sleep(2)\n",
    "    print(\"soup\")\n",
    "    if driver.current_url != 'https://anilist.co/404':\n",
    "        try:\n",
    "            popup_html = driver.page_source\n",
    "\n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            popup_soup = BeautifulSoup(popup_html, 'html.parser')\n",
    "\n",
    "            ## Scraping relevant information from the popup using BeautifulSoup\n",
    "            # Find all <a> tags with the desired class\n",
    "            genre_tags = popup_soup.find_all('a', class_='el-tooltip name')\n",
    "\n",
    "            # Find all <div> tags with the class \"rank\"\n",
    "            rank_tags = popup_soup.find_all('div', class_='rank')\n",
    "\n",
    "            # Extract the title of the anime\n",
    "            title = popup_soup.find(\"h1\").get_text(strip=True)\n",
    "\n",
    "            # Extract the description/synopsis\n",
    "            des = popup_soup.find(\"p\", class_=\"description content-wrap\").get_text(strip=True)\n",
    "\n",
    "            if title != None and des != None and  genre_tags != None and  rank_tags != None:\n",
    "                # make list to save the info\n",
    "                Genre = []\n",
    "                rank = []\n",
    "\n",
    "                # Extract and print the text content of each <a> tag\n",
    "                for tag in genre_tags:\n",
    "                    genre = tag.get_text(strip=True)\n",
    "                    Genre.append(genre)\n",
    "\n",
    "                # Extract and print the text content of rank tags\n",
    "                for rank_tag in rank_tags:\n",
    "                    rank_text = rank_tag.get_text(strip=True)\n",
    "                    rank.append(rank_text)\n",
    "                genres = dict(zip(Genre,rank))\n",
    "\n",
    "                return title, des, genres\n",
    "        \n",
    "        except:\n",
    "            sleep(3)\n",
    "            soup(driver)\n",
    "    else:\n",
    "        return \"Error404\",None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(website_url, driver):\n",
    "    \n",
    "    driver.get(website_url)\n",
    "    sleep(3)\n",
    "    print(\"scrape\")\n",
    "    # Extracting links from site\n",
    "    links = driver.find_elements(By.CLASS_NAME, 'cover-link')\n",
    "    urls = [link.get_attribute('href') for link in links]\n",
    "    print(urls)\n",
    "    urls = [link for link in links if \"anime/\" in link]\n",
    "    print(urls)\n",
    "\n",
    "    try:\n",
    "        # Click the button\n",
    "        button = driver.find_element(By.CLASS_NAME, \"spoiler-toggle\")\n",
    "        button.click()\n",
    "        sleep(2)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        title, des, genre = soup(driver)\n",
    "    except:\n",
    "        title, des, genre = None, None, None\n",
    "    \n",
    "    print(title, des, genre, urls)\n",
    "    return title, des, genre, urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(new_data, filename=\"Anime_data.json\"):\n",
    "    \n",
    "    # Read the existing JSON data from the file\n",
    "    existing_data = []\n",
    "    with open(filename,'r',encoding='utf-8') as file:\n",
    "        try:\n",
    "            # First we load existing data into a dict.\n",
    "            existing_data = json.load(file)\n",
    "        except:\n",
    "            existing_data = []\n",
    "    file.close()\n",
    "    # Join new_data with existing_data\n",
    "    existing_data.append(new_data)\n",
    "\n",
    "    with open(filename,\"w\",encoding='utf-8') as file:\n",
    "        \n",
    "        # convert back to json.\n",
    "        json.dump(existing_data, file,ensure_ascii=False, indent = 4)\n",
    "    \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(driver, unvisited_sites, visited_sites):\n",
    "\n",
    "    # Making a List of website to visit using BSF\n",
    "    to_visit = list(unvisited_sites - visited_sites)\n",
    "    print(to_visit)\n",
    "    # Loop to Scrape one set of to_visit links\n",
    "    for url in to_visit:\n",
    "        title, des, genres, urls = scrape(url,driver=driver)\n",
    "        sleep(2)\n",
    "        if title != \"Error404\":\n",
    "            # Saving them in an list to further save it in Json\n",
    "            Anime_data = {\n",
    "                \"Anime\" : {\n",
    "                \"Anime_Title\" : title,\n",
    "                \"Description\" : des,\n",
    "                \"Tags\" : genres\n",
    "                },\n",
    "                \"unvisited\" : urls,\n",
    "                \"visited\" : url\n",
    "            }\n",
    "\n",
    "            # Injecting scraped data into json\n",
    "            write_json(Anime_data,\"Anime_data.json\")\n",
    "\n",
    "            # Updating the unvisited and visited sets\n",
    "            if unvisited_sites != None or unvisited_sites != 'null':\n",
    "                unvisited_sites.update(urls)\n",
    "            visited_sites.add(url)\n",
    "        else:\n",
    "            visited_sites.add(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(filename='sites_data.json'):\n",
    "    # Load data from the JSON file\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        site_data = json.load(file)\n",
    "\n",
    "    # Extract the lists from the loaded data\n",
    "    try:\n",
    "        visited_sites_list = site_data.get(\"visited_sites\", [])\n",
    "    except:\n",
    "        visited_sites_list = []\n",
    "    try:\n",
    "        unvisited_sites_list = site_data.get(\"unvisited_sites\", [])\n",
    "    except:\n",
    "        visited_sites_list = []\n",
    "    \n",
    "    return visited_sites_list, unvisited_sites_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a set to keep visited & unvisited sites\n",
    "visited_sites, unvisited_sites = load_files()\n",
    "visited_sites, unvisited_sites = set(visited_sites), set(unvisited_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://anilist.co/anime/21/ONE-PIECE/']\n",
      "scrape\n",
      "['https://anilist.co/anime/11061/HUNTERHUNTER-2011/', 'https://anilist.co/anime/20/NARUTO/', 'https://anilist.co/anime/1735/NARUTO-Shippuuden/', 'https://anilist.co/anime/97940/Black-Clover/']\n"
     ]
    }
   ],
   "source": [
    "# Initializing Driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Your crawling logic here\n",
    "driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "try:\n",
    "    # visting and scraping sites\n",
    "    while True:\n",
    "        crawler(driver,unvisited_sites, visited_sites)\n",
    "except:\n",
    "    # Create a dictionary to store both sets\n",
    "    site_data = {\n",
    "    \"visited_sites\": list(visited_sites),\n",
    "    \"unvisited_sites\": list(unvisited_sites)\n",
    "    }\n",
    "\n",
    "    # Save the dictionary to a JSON file\n",
    "    with open('sites_data.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(site_data, file)\n",
    "\n",
    "# Close the driver properly\n",
    "# driver.close()\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
